# -*- coding: utf-8 -*-
"""twitter_api_io_test

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/embedded/projects/bigtimestudios/locations/us-central1/repositories/2df84163-17f5-4830-8f62-b6b087aceabc
"""

# !pip install --upgrade crewai pydantic loguru google-cloud-bigquery -q
# import sys
# !{sys.executable} -m pip install -U pip
# !{sys.executable} -m pip install -U litellm "crewai>=0.46.0" google-cloud-aiplatform google-auth
# 1) Clean out conflicting OTEL packages (ignore errors if some aren't installed)
!pip uninstall -y opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp opentelemetry-exporter-otlp-proto-grpc opentelemetry-exporter-otlp-proto-http opentelemetry-proto opentelemetry-semantic-conventions

# 2) Install a consistent set (works well together)
!pip install -U "opentelemetry-api==1.25.0" "opentelemetry-sdk==1.25.0" "opentelemetry-exporter-otlp==1.25.0" "opentelemetry-proto==1.25.0" "opentelemetry-semantic-conventions==0.46b0"

# (Optional) Upgrade crewai to a recent version too
!pip install -U crewai

from opentelemetry.util import types
assert hasattr(types, "_ExtendedAttributes"), "OTel still mismatched"
from crewai import Agent, Crew, Process, Task, LLM  # should import cleanly now

import sys
# Clean install (idempotent)
!{sys.executable} -m pip install -U pip
!{sys.executable} -m pip install -U litellm "crewai>=0.46.0" google-cloud-aiplatform google-auth

import pandas as pd
from google.cloud import bigquery
from google.auth import default
import requests
import time

"""### Get users"""

# set up configuration
credentials, project_id = default()
client = bigquery.Client(credentials=credentials, project=project_id)

# link to key: https://twitterapi.io/dashboard
# link to documentation: https://docs.twitterapi.io/introduction

API_KEY = '364eae94e2c24308af03ea2489f5f41e'
API_KEY = '2ed4b1a59b6e443185d2b2e887de545a'

HEADERS = {
    "X-API-Key": API_KEY,
    "Content-Type": "application/json"
}

df = client.query('''
with external_accounts as (
  select
  user_id,
  COALESCE(
    JSON_VALUE(raw, '$.screen_name'),
    JSON_VALUE(SAFE.PARSE_JSON(JSON_VALUE(raw, '$')), '$.screen_name')
  ) AS screen_name,
  REPLACE(REGEXP_REPLACE(to_json_string(raw), r'^"|"$', ''), r'\"', '"') AS raw_unescaped,
  raw,
  CAST(JSON_EXTRACT_SCALAR(raw, '$.followers_count') AS INT64) AS followers_count,
  from openloot-362008.postgres_rds_auth_api_public.external_accounts
  where provider='twitter'

)

select
user_id,
json_extract_scalar(raw_unescaped,"$.id") twitter_id,
coalesce(screen_name,
        json_extract_scalar(raw_unescaped,"$.screen_name")
        ) screen_name,
followers_count
from external_accounts
order by followers_count desc
''').to_dataframe()

df.head(3)

df_users = client.query('''
  select
  *
  from openloot-362008.trump_ml.risk_users_beta
''').to_dataframe()

df_users['username'] = df_users['username'].str.replace(r"[\"']", "", regex=True)
df_users.head()

"""#### Get tweets"""

import requests
import pandas as pd

API_KEY = '2ed4b1a59b6e443185d2b2e887de545a'
HEADERS = {"x-api-key": API_KEY}

def get_tweets_df(username):
    url = "https://api.twitterapi.io/twitter/user/last_tweets"
    params = {"userName": username.lstrip("@")}
    r = requests.get(url, headers=HEADERS, params=params, timeout=30)
    r.raise_for_status()
    data = r.json()

    # normalize structure
    tweets = data.get("tweets") or data.get("data") or []
    if isinstance(tweets, dict):
        tweets = (
            tweets.get("tweets")
            or tweets.get("items")
            or [tweets.get("pinned_tweet") or tweets.get("pin_tweet")]
        )
    if not isinstance(tweets, list):
        tweets = []

    # extract fields
    rows = []
    for t in tweets:
        if not isinstance(t, dict):
            rows.append({"username": username, "timestamp": None, "text": str(t)})
            continue
        text = (
            t.get("text")
            or t.get("full_text")
            or t.get("legacy", {}).get("full_text")
            or t.get("tweet", {}).get("text")
            or ""
        )
        ts = (
            t.get("created_at")
            or t.get("createdAt")
            or t.get("timestamp")
            or t.get("legacy", {}).get("created_at")
            or t.get("tweet", {}).get("created_at")
        )
        rows.append({"username": username, "timestamp": ts, "text": text})

    return pd.DataFrame(rows, columns=["username", "timestamp", "text"])

# Example
df = get_tweets_df("_HD_Unlimited")
df.head()

import pandas as pd
from datetime import datetime

def prepare_user_corpus(tweets_df: pd.DataFrame,
                        max_tweets_per_user: int = 400,
                        max_chars: int = 15000) -> pd.DataFrame:
    """
    Normalize columns, aggregate all tweets per user into a single text blob.
    """
    df = tweets_df.rename(columns={"username":"twitterusername",
                                   "timestamp":"timestamp",
                                   "text":"text"}).copy()

    # basic cleanup
    df["twitterusername"] = (df["twitterusername"].astype(str)
                             .str.strip().str.lstrip("@").str.strip('"').str.strip("'"))
    df["text"] = df["text"].astype(str).fillna("")

    # (optional) cap per-user tweet count to avoid huge prompts
    df = (df.sort_values(["twitterusername","timestamp"])
            .groupby("twitterusername")
            .head(max_tweets_per_user))

    # aggregate tweets → one big string per user
    agg = (df.groupby("twitterusername")["text"]
             .apply(lambda s: "\n".join(s.astype(str).tolist())[:max_chars])
             .reset_index()
             .rename(columns={"text":"tweets_content"}))

    # add metadata used by tasks
    agg["analysis_date"] = str(datetime.now().date())
    agg["user_handle"] = "@" + agg["twitterusername"]
    return agg[["twitterusername","user_handle","analysis_date","tweets_content"]]

tweets_df = prepare_user_corpus(df)
tweets_df.head()

"""#### Agents"""

from crewai import Agent, Crew, Process, Task, LLM
from pydantic import BaseModel, field_validator
from typing import Optional, Union
import pandas as pd
import json
import glob
from datetime import datetime
from tqdm import tqdm
import warnings
from typing import Dict, Optional, Union
import litellm
import crewai.llm as cllm
import os, crewai.llm as cllm, importlib
cllm = importlib.reload(cllm)

warnings.filterwarnings("ignore")
cllm = importlib.reload(cllm)

# VERTEX_LLM = cllm.LLM(
#     model="vertex_ai/gemini-2.5-pro",
#     is_litellm=True,
#     temperature=0.7,
#     top_p=0.95,
# )

# use the lighter model
# os.environ["VERTEXAI_PROJECT"]  = "bigtimestudios"
# os.environ["VERTEXAI_LOCATION"] = "us-central1"

VERTEX_LLM = cllm.LLM(
    model="gemini-2.0-flash-lite-001",
    is_litellm=True,
    temperature=0.2,
    top_p=0.9,
)

print("CrewAI LLM ready")

class RiskAnalysisOutput(BaseModel):
    # User identification
    handle: str

    # Individual domain summaries
    trump_summary: str
    crypto_summary: str
    gaming_summary: str

    # Individual domain scores
    trump_score: float
    crypto_score: float
    gaming_score: float

    # Individual domain sentiments
    trump_sentiment: Optional[str] = None
    crypto_sentiment: Optional[str] = None
    gaming_sentiment: Optional[str] = None

    # Individual domain risks
    trump_risk: str
    crypto_risk: str
    gaming_risk: str

    # Individual domain confidence levels
    trump_confidence: float
    crypto_confidence: float
    gaming_confidence: float

    # Individual domain evidence counts
    trump_evidence_count: int
    crypto_evidence_count: int
    gaming_evidence_count: int

    # Aggregated results
    overall_risk: str
    recommendation: str
    overall_confidence: float

    # Metadata
    analysis_timestamp: str
    validation_status: str

    # Optional validation details
    trump_validation_notes: Optional[str] = None
    crypto_validation_notes: Optional[str] = None
    gaming_validation_notes: Optional[str] = None
    trump_content_validation: Optional[Union[str, dict]] = None
    crypto_content_validation: Optional[Union[str, dict]] = None
    gaming_content_validation: Optional[Union[str, dict]] = None

    @field_validator("trump_sentiment", "crypto_sentiment", "gaming_sentiment", mode="before")
    @classmethod
    def coerce_sentiment(cls, v):
        if v is None or (isinstance(v, str) and v.strip() == ""):
            return "neutral"
        return str(v)

    @field_validator(
        "trump_content_validation",
        "crypto_content_validation",
        "gaming_content_validation",
        mode="before",
    )
    @classmethod
    def coerce_content_validation(cls, v):
        if v is None:
            return None
        if isinstance(v, dict):
            return json.dumps(v, ensure_ascii=False)
        return str(v)


from crewai import Agent, Task, Crew, Process

agent = Agent(
    role="Tester",
    goal="Return a short sentence.",
    backstory="Just a sanity check.",
    llm=VERTEX_LLM,
)

task = Task(
    description="Reply with the word 'pong' and nothing else.",
    expected_output="pong",   # <-- REQUIRED in 1.4.x
    agent=agent,
)

crew = Crew(
    agents=[agent],
    tasks=[task],
    process=Process.sequential,
    verbose=True,            # optional: see logs
)

result = crew.kickoff()
print(result if isinstance(result, str) else getattr(result, "raw", result))

DEFAULT_LLM = VERTEX_LLM

def create_agents(llm=DEFAULT_LLM, verbose: bool = True) -> Dict[str, Agent]:
    agents: Dict[str, Agent] = {}
    agents['trump_agent'] = Agent(
        role="""Political Sentiment Analysis Specialist focusing on Trump-related discourse
""",
        goal="""Analyze Twitter content to determine user sentiment towards Trump. Produce a structured analysis with key signals, confidence levels, and actionable summaries. Ensure the output is objective and based solely on the content provided. Avoid assumptions and do not rely on external facts. Your output
will feed a downstream risk aggregation system — therefore, consistency and structure are crucial
""",
        backstory="""You are a senior political analyst with 15+ years of experience in sentiment analysis, public opinion research, and social media behavior. You specialize in interpreting nuanced political rhetoric, dogwhistles, culture-war narratives,
and polarizing discourse. You combine linguistic analysis, context retrieval, and pattern recognition to distinguish genuine political support from irony, troll behavior, or engagement farming. You produce structured, precise, and defensible assessments,
communicating sentiment clearly with confidence levels and a rationale grounded in the content provided — not speculation, not external knowledge. Focus on internal consistency and coherence. Avoid assumptions not supported by the tweets. Your output should
be concise, clearly structured, and useful for risk evaluation pipelines. You maintain high standards of clarity and integrity, and you are not swayed by personal political bias. You provide  evidence-based assessments with clear confidence levels.
""",
        llm=llm,
        allow_delegation=False,
        verbose=verbose,
    )
    agents['crypto_agent'] = Agent(
        role="""Cryptocurrency and Web3 Sentiment Analysis Specialist
""",
        goal="""Evaluate user sentiment towards cryptocurrency, blockchain, and Web3, identifying whether the user appears positively inclined towards crypto or expresses skepticism, hostility, or misinformation-prone opinions. Distinguish between informed commentary
and low-effort anti-crypto rhetoric. Produce a structured analysis with key signals, confidence levels, and actionable summaries. Ensure the output is objective and based solely on the content provided. Avoid assumptions and do not rely on external facts. Your output
will feed a downstream risk aggregation system — therefore, consistency and structure are crucial.
""",
        backstory="""You are a cryptocurrency market analyst and blockchain research specialist, experienced in tracking public sentiment across Twitter and crypto communities. You are skilled at identifying common narratives (e.g., "crypto is a scam", "NFTs are dead",
"Web3 gaming is P2W", "only Bitcoin is real", "DeFi is dangerous") and separating constructive skepticism from broad hostility. Your analysis considers tone, vocabulary, intent, and coherence, with attention to whether the user demonstrates understanding of basic
crypto concepts (wallets, tokens, NFTs, DeFi, layer-2s). You strive for objectivity and clarity. You do not overgeneralize — you extract precise assessments with confidence levels. Your outputs are structured and suitable for downstream pipelines. The goal is to identify
patterns in sentiment (pro/anti/neutral), substantive reasoning vs. slogans, and the difference between informed crypto opinions and uninformed criticism.
""",
        llm=llm,
        allow_delegation=False,
        verbose=verbose,
    )
    agents['gaming_agent'] = Agent(
        role="""Web3 Gaming and NFT Sentiment Analysis Specialist
""",
        goal="""Assess user sentiment towards web3 gaming, NFTs, and play-to-earn models, identifying support, skepticism, or hostility. Distinguish between constructive game design criticism and blanket anti-web3 attitudes. Provide structured findings with confidence
levels and concise summaries tailored for risk evaluation workflows.
""",
        backstory="""You are a gaming industry analyst specializing in player behavior, monetization models, and web3-integrated game economies. You understand the difference between critiques of gameplay quality vs. ideological opposition to NFTs or tokens in games.
You evaluate whether the user demonstrates informed opinions about web3 gaming (ownership, interoperability, tokenomics) or repeats common anti-web3 talking points without substance. Your assessments are balanced, precise, and structured for downstream processing.
You communicate clearly and concisely, with an emphasis on evidence-based reasoning. You can differentiate between sarcasm, memes, and explicit statements. You maintain neutrality while identifying risk signals around anti-web3 activism, boycott language,
and brigading behavior. You are not swayed by hype — you focus on text evidence only and produce clear, structured outputs.
""",
        llm=llm,
        allow_delegation=False,
        verbose=verbose,
    )
    agents['trump_validator'] = Agent(
        role="""Political Sentiment Validation Specialist
""",
        goal="""Validate and quality-check Trump sentiment analysis for accuracy, internal consistency, and alignment with the source tweets. Ensure clarity of the final labeling (pro/anti/neutral), confidence levels, and justification. Identify misclassifications,
overreach, or bias. Produce a corrected/validated output where needed.
""",
        backstory="""You are a quality assurance expert in political sentiment analysis with rigorous standards for evidence and reproducibility. You carefully review the original tweets and compare them to the analysis output, checking for fidelity, overstatements,
and missing nuance. You correct errors, clarify ambiguous statements, and ensure the output adheres to the required JSON schema and downstream compatibility. You are detail-oriented and precise, with strong editorial judgment and ability to catch  discrepancies that others might miss.
""",
        llm=llm,
        allow_delegation=False,
        verbose=verbose,
    )
    agents['crypto_validator'] = Agent(
        role="""Cryptocurrency Analysis Validation Specialist
""",
        goal="""Validate crypto sentiment analysis outputs, ensuring that claims are supported by the tweets content, that confidence levels are justified, and that the structure conforms to schema requirements. Correct misinterpretations or weak assumptions, and refine
risk assessments according to established  validation protocols
""",
        backstory="""You are a cryptocurrency data validation expert with deep knowledge of common narratives, misinformation patterns, and sentiment cues in crypto discourse. You ensure that the analysis does not overreach or speculate beyond the text evidence. You are meticulous
and systematic, providing clear, actionable corrections. You prioritize reproducibility and schema compliance. You are patient, thorough, and able to identify  inconsistencies in analysis results.
""",
        llm=llm,
        allow_delegation=False,
        verbose=verbose,
    )
    agents['gaming_validator'] = Agent(
        role="""Gaming Sentiment Validation Specialist
""",
        goal="""Validate gaming sentiment analysis to ensure it aligns with source tweets, maintains balanced reasoning, and correctly distinguishes gameplay critiques from anti-web3 sentiment. Confirm structure and clarity according to schema, and refine confidence levels.
""",
        backstory="""You are a gaming industry data validation specialist with expertise in content review, annotation guidelines, and schema enforcement. You verify that the analysis is faithful to the tweets, logically coherent, and useful for downstream risk aggregation.
You provide targeted corrections and clarifications when necessary. You care about precision, fairness, and the ability to maintain high standards  in gaming-related data analysis.
""",
        llm=llm,
        allow_delegation=False,
        verbose=verbose,
    )
    agents['risk_aggregator'] = Agent(
        role="""Multi-Domain Risk Assessment Aggregator
""",
        goal="""Synthesize validated sentiment analysis results from Trump, crypto, and gaming domains into a unified RiskAnalysisOutput. Produce a final, downstream-ready JSON object with an overall risk level, per-domain assessments, and clear rationale. Ensure consistency,
traceability, and completeness. Deliver a succinct summary of key drivers of risk, confidence levels, and final  recommendations for user acceptance or rejection
""",
        backstory="""You are a senior risk assessment analyst with expertise in multi-domain synthesis and decision frameworks. You combine validated results to produce a coherent, actionable final output. You maintain rigorous standards for structure and clarity. You do not add
new assumptions; you only aggregate and reconcile validated findings. You are skilled at identifying alignment or conflicts between domain analyses and translating them into a clean, consistent RiskAnalysisOutput schema. You communicate trade-offs, confidence, and next steps.
You understand how to synthesize complex information  into clear, actionable insights.
""",
        llm=llm,
        allow_delegation=False,
        verbose=verbose,
    )
    return agents


def create_tasks(agents: Dict[str, Agent]) -> Dict[str, Task]:
    tasks: Dict[str, Task] = {}
    # Task: trump_analysis_task
    tasks['trump_analysis_task'] = Task(
        description="""Analyze the provided Twitter content to assess user sentiment towards Donald Trump. Identify explicit or implicit pro/anti/neutral signals and produce a structured, evidence-based analysis suitable for downstream risk evaluation.

TWITTER CONTENT TO ANALYZE:
{tweets_content}

USER HANDLE: {user_handle}
ANALYSIS DATE: {analysis_date}

Your analysis should address:
1. Whether the user expresses explicit support for Trump (e.g., praise, defense, agreement with his positions)
2. Whether the user expresses explicit opposition to Trump (e.g., criticism, ridicule, advocacy against him)
3. Whether sentiment is neutral/ambiguous or mixed
4. Whether there are dogwhistles, slogans, or coded language suggesting strong ideological positions
5. Whether the content includes sarcasm, irony, or meme-based messaging that changes sentiment interpretation
6. A short rationale connecting tweet evidence to the sentiment label

OUTPUT SCHEMA (strict JSON):
{
  "domain": "trump",
  "sentiment": "pro" | "anti" | "neutral",
  "confidence": 0.0-1.0,
  "signals": [ "short bullet on signal 1", "short bullet on signal 2", ... ],
  "evidence_tweets": [ "paste the subset of tweets you used as the main evidence (text only)" ],
  "rationale": "2-4 sentences explaining how the evidence supports the sentiment and confidence"
}

Rigidly follow the schema. Do not invent tweets; only use provided content. Keep it concise and factual.
""",
        expected_output="""A JSON object as per the schema, with sentiment, confidence, signals, evidence_tweets, and rationale fully populated and consistent with the provided tweets. The analysis should be faithful to the text evidence and avoid speculation.
""",
        agent=agents['trump_agent'],
    )

    # Task: trump_validation_task
    tasks['trump_validation_task'] = Task(
        description="""Validate the Trump sentiment analysis output for accuracy, fidelity to the tweets, and schema compliance.

Validation procedure:
1) Compare the analysis to the source tweets and confirm that sentiment and evidence align.
2) Check that confidence is justified by the strength/clarity of evidence.
3) Ensure that signals and rationale are specific, concise, and relevant.
4) Correct any misclassifications or overreach; fix schema violations.

Return a corrected/validated JSON object with the same schema as the analysis output.
""",
        expected_output="""A JSON object with fields: domain, sentiment, confidence, signals, evidence_tweets, rationale — validated and corrected if needed. Keep changes minimal and justified.
""",
        agent=agents['trump_validator'],
        context=[tasks['trump_analysis_task']],
    )

    # Task: crypto_analysis_task
    tasks['crypto_analysis_task'] = Task(
        description="""Analyze the provided Twitter content to assess user sentiment towards cryptocurrency, blockchain, and Web3.

TWITTER CONTENT TO ANALYZE:
{tweets_content}

USER HANDLE: {user_handle}
ANALYSIS DATE: {analysis_date}

Your analysis should identify:
1. Whether the user is pro-crypto, anti-crypto, or neutral
2. Whether the user demonstrates basic understanding of crypto concepts
3. Key narratives present (e.g., "crypto is a scam", "NFTs are dead", "BTC maxis", "DeFi risky")
4. Any investment attitude signals (bullish, bearish, skeptical but open-minded)
5. A concise rationale linked to evidence from the tweets

OUTPUT SCHEMA (strict JSON):
{
  "domain": "crypto",
  "sentiment": "pro" | "anti" | "neutral",
  "confidence": 0.0-1.0,
  "signals": [ "signal 1", "signal 2", ... ],
  "evidence_tweets": [ "paste the subset of tweets used as evidence" ],
  "rationale": "2-4 sentences with clear justification"
}
""",
        expected_output="""A JSON object with the fields exactly as defined above, with clear, concise, and evidence-based content suitable for downstream validation and risk aggregation.
""",
        agent=agents['crypto_agent'],
    )

    # Task: crypto_validation_task
    tasks['crypto_validation_task'] = Task(
        description="""Validate the crypto sentiment analysis for evidence alignment, schema compliance, and clarity.

Validation includes:
- Cross-checking sentiment with provided tweets
- Adjusting confidence based on strength of evidence
- Tightening signals/rationale for precision
- Correcting any schema issues

Return a corrected/validated JSON object maintaining the same schema.
""",
        expected_output="""A JSON object with the same structure as the crypto analysis but validated/corrected for fidelity, clarity, and schema compliance.
""",
        agent=agents['crypto_validator'],
        context=[tasks['crypto_analysis_task']],
    )

    # Task: gaming_analysis_task
    tasks['gaming_analysis_task'] = Task(
        description="""Analyze the provided Twitter content to assess user sentiment towards web3 gaming,
NFTs, and blockchain-based gaming platforms.

TWITTER CONTENT TO ANALYZE:
{tweets_content}

USER HANDLE: {user_handle}
ANALYSIS DATE: {analysis_date}

Your analysis should focus on:
1. Identifying tweets that mention web3 gaming, NFT games, play-to-earn, or gaming tokens
2. Determining sentiment polarity (pro-gaming, anti-gaming, neutral)
3. Assessing understanding of gaming monetization and player economics
4. Identifying any ideological opposition vs. gameplay-quality critiques
5. Providing a concise rationale linked to tweet evidence

OUTPUT SCHEMA (strict JSON):
{
  "domain": "gaming",
  "sentiment": "pro" | "anti" | "neutral",
  "confidence": 0.0-1.0,
  "signals": [ "signal 1", "signal 2", ... ],
  "evidence_tweets": [ "paste evidence tweets" ],
  "rationale": "2-4 sentence justification"
}
""",
        expected_output="""A JSON object with the following structure:
{
  "domain": "gaming",
  "sentiment": "pro" | "anti" | "neutral",
  "confidence": 0.0-1.0,
  "signals": [ "signal 1", "signal 2", ... ],
  "evidence_tweets": [ "paste evidence tweets" ],
  "rationale": "2-4 sentence justification",
  "validated": false
}
Note: field "validated" should be included: always false initially (will be validated by next agent)
""",
        agent=agents['gaming_agent'],
    )

    # Task: gaming_validation_task
    tasks['gaming_validation_task'] = Task(
        description="""Validate the gaming analysis for fidelity to tweets, clear distinction between gameplay critiques and anti-web3 ideology, and schema compliance.

Perform validation including content accuracy checks, confidence adjustment, and minimal corrections for clarity. Return a corrected/validated object.
""",
        expected_output="""A JSON object with the same structure as the gaming analysis, but with:
- validated set to true
- any necessary corrections to sentiment, confidence, signals
- a brief "validation" note string, e.g., "Analysis accurately reflects tweet content"
""",
        agent=agents['gaming_validator'],
        context=[tasks['gaming_analysis_task']],
    )

    # Task: risk_aggregation_task
    tasks['risk_aggregation_task'] = Task(
        description="""Synthesize the validated outputs from trump, crypto, and gaming domains into a single RiskAnalysisOutput.

You must:
1) Merge domain-level sentiments and confidences
2) Derive an overall risk rating (low/medium/high) based on a simple rule:
   - high if any domain is "anti" with confidence >= 0.7
   - medium if any domain is "anti" with confidence between 0.4 and 0.69
   - otherwise low
3) Provide a short final rationale referencing domain results
4) Include validation notes where relevant
5) Output must be a clean JSON object ready for downstream consumption

OUTPUT SCHEMA:
{
  "overall_risk": "low" | "medium" | "high",
  "domains": {
    "trump": { ...validated trump object... },
    "crypto": { ...validated crypto object... },
    "gaming": { ...validated gaming object... }
  },
  "rationale": "short paragraph explaining the result",
  "timestamp": "{analysis_date}",
  "user_handle": "{user_handle}"
}

Use only the validated objects (not the raw analyses). Include the user handle from the original analysis context.
""",
        expected_output="""A complete RiskAnalysisOutput object with all required fields populated from validation tasks.
""",
    agent=agents['risk_aggregator'],
    context=[tasks['trump_validation_task'], tasks['crypto_validation_task'], tasks['gaming_validation_task']],
    output_pydantic=RiskAnalysisOutput,
    )

    return tasks


def create_crew_sequential(agents: Dict[str, Agent], tasks: Dict[str, Task], verbose: bool = True) -> Crew:
    # Preserva a ordem pensada no YAML para execução determinística
    ordered_agent_list = [agents[k] for k in [
        'trump_agent',
        'crypto_agent',
        'gaming_agent',
        'trump_validator',
        'crypto_validator',
        'gaming_validator',
        'risk_aggregator',
    ] if k in agents]
    ordered_task_list = [tasks[k] for k in [
        'trump_analysis_task',
        'trump_validation_task',
        'crypto_analysis_task',
        'crypto_validation_task',
        'gaming_analysis_task',
        'gaming_validation_task',
        'risk_aggregation_task',
    ] if k in tasks]

    return Crew(
        agents=ordered_agent_list,
        tasks=ordered_task_list,
        process=Process.sequential,
        verbose=verbose,
    )


agents = create_agents()
tasks = create_tasks(agents)
crew = create_crew_sequential(agents, tasks)

from typing import Dict, Any, List
import json
import math

# sentiment → signed score helper
def _sentiment_to_score(sentiment: str, confidence: float) -> float:
    s = (sentiment or "").lower()
    c = float(confidence) if confidence is not None and not pd.isna(confidence) else 0.0
    if s == "pro":
        return +c
    if s == "anti":
        return -c
    return 0.0  # neutral/unknown

def _extract_domain(payload: Dict[str,Any], domain: str) -> Dict[str,Any]:
    """
    Accepts either a flat object or nested {"domains": {"trump": {...}, ...}}
    Returns {'sentiment': str, 'confidence': float} for the requested domain.
    """
    d = None
    # nested
    if isinstance(payload.get("domains"), dict):
        d = payload["domains"].get(domain)
    # flat fallback (rare): try top-level keys like 'trump_sentiment'
    if d is None:
        sent = payload.get(f"{domain}_sentiment")
        conf = payload.get(f"{domain}_confidence")
        return {"sentiment": sent, "confidence": conf}

    if not isinstance(d, dict):
        return {"sentiment": None, "confidence": None}

    return {"sentiment": d.get("sentiment"),
            "confidence": d.get("confidence")}

def _normalize_result(result_obj: Any) -> Dict[str,Any]:
    """
    Convert Crew result -> dict safely (pydantic v1/v2/string).
    """
    # pydantic v2
    try:
        return result_obj.model_dump()
    except Exception:
        pass
    # pydantic v1
    try:
        return result_obj.dict()
    except Exception:
        pass
    # JSON string
    if isinstance(result_obj, str):
        try:
            return json.loads(result_obj)
        except Exception:
            return {"raw": result_obj}
    # last resort
    return json.loads(json.dumps(result_obj, default=str))

def analyze_users_with_crew(tweets_df: pd.DataFrame) -> pd.DataFrame:
    """
    Returns dataframe with: twitterusername, trump_score, crypto_score, gaming_score
    based on your existing agents/tasks structure.
    """
    # 1) aggregate tweets per user
    agg_df = prepare_user_corpus(tweets_df)

    # 2) build crew once
    agents = create_agents(llm=VERTEX_LLM, verbose=False)
    tasks  = create_tasks(agents)
    crew   = create_crew_sequential(agents, tasks, verbose=False)

    rows: List[Dict[str,Any]] = []

    for _, row in agg_df.iterrows():
        inputs = {
            "tweets_content": row["tweets_content"],
            "user_handle": row["user_handle"],
            "analysis_date": row["analysis_date"],
        }

        # 3) run the full pipeline for this user
        result = crew.kickoff(inputs=inputs)
        data   = _normalize_result(result)

        # 4) pull domain-level outputs
        trump  = _extract_domain(data, "trump")
        crypto = _extract_domain(data, "crypto")
        gaming = _extract_domain(data, "gaming")

        # 5) compute signed scores from sentiment+confidence
        trump_score  = _sentiment_to_score(trump.get("sentiment"),  trump.get("confidence"))
        crypto_score = _sentiment_to_score(crypto.get("sentiment"), crypto.get("confidence"))
        gaming_score = _sentiment_to_score(gaming.get("sentiment"), gaming.get("confidence"))

        rows.append({
            "twitterusername": row["twitterusername"],
            "trump_score":  trump_score,
            "crypto_score": crypto_score,
            "gaming_score": gaming_score,
          #  (optional debug columns—comment out if you want only scores)
            "trump_sentiment":  trump.get("sentiment"),
            "crypto_sentiment": crypto.get("sentiment"),
            "gaming_sentiment": gaming.get("sentiment"),
            "trump_confidence":  trump.get("confidence"),
            "crypto_confidence": crypto.get("confidence"),
            "gaming_confidence": gaming.get("confidence"),
        })

    return pd.DataFrame(rows, columns=["twitterusername","trump_score","crypto_score","gaming_score", "trump_sentiment","crypto_sentiment", "gaming_sentiment", "trump_confidence",  "crypto_confidence", "gaming_confidence"])

"""#### Run Analysis"""

# tweets_df must have: username, timestamp, text
# Build tweets_df using your fetcher, then:
result_df = analyze_users_with_crew(df)
print(result_df.head())

result_df.head()

