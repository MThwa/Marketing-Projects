{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "from datetime import date, timedelta\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from google.auth import default\n",
        "import numpy as np\n",
        "import json\n"
      ],
      "metadata": {
        "id": "1US4aD7NuABB"
      },
      "id": "1US4aD7NuABB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Config & clients ----------\n",
        "PROJECT_ID = \"bigtimestudios\"\n",
        "LOCATION = \"us-central1\"\n",
        "credentials, project = default()\n",
        "client = bigquery.Client(credentials=credentials, project=project)\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "model = GenerativeModel(\"gemini-2.5-pro\")"
      ],
      "metadata": {
        "id": "hdqnt497uB4S"
      },
      "id": "hdqnt497uB4S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Dates ----------\n",
        "RUN_DATE = date.today() - timedelta(days=2)   # your temp override\n",
        "YDAY     = RUN_DATE - timedelta(days=1)\n",
        "WEEK_AGO = RUN_DATE - timedelta(days=7)\n",
        "DAYS30_AGO = RUN_DATE - timedelta(days=30)\n",
        "LOOKBACK = RUN_DATE - timedelta(days=40)      # cushion for gaps\n",
        "\n",
        "RUN_DATE_STR = RUN_DATE.strftime(\"%Y-%m-%d\")"
      ],
      "metadata": {
        "id": "N-lxWYu2uDl3"
      },
      "id": "N-lxWYu2uDl3",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pct(a, b):\n",
        "    if b is None or pd.isna(b) or b == 0: return np.nan\n",
        "    return (a - b) / b\n",
        "\n",
        "def ensure_date(col):\n",
        "    return pd.to_datetime(col).dt.date\n",
        "\n",
        "def window_avg(df, date_col: str, value_col: str, start_date, end_date):\n",
        "    \"\"\"Mean of daily totals in [start_date, end_date).\"\"\"\n",
        "    win = df[(df[date_col] >= start_date) & (df[date_col] < end_date)]\n",
        "    if win.empty: return np.nan\n",
        "    daily = win.groupby(date_col, as_index=False)[value_col].sum()\n",
        "    return float(daily[value_col].mean())\n",
        "\n",
        "def change_vs_prior(current: pd.DataFrame,\n",
        "                    prior: pd.DataFrame,\n",
        "                    hist: pd.DataFrame,\n",
        "                    date_col: str,\n",
        "                    keys: list[str],\n",
        "                    metric: str,\n",
        "                    top_n: int | None = None,\n",
        "                    ascending: bool = False) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Movers with DoD delta plus 7d/30d baselines (mean of daily totals) and % vs baselines.\n",
        "    hist must include at least 30d before RUN_DATE.\n",
        "    \"\"\"\n",
        "    # group today + prior\n",
        "    if current is None or current.empty:\n",
        "        cols = keys + [metric, f\"{metric}_yday\", \"delta\", \"pct_change\",\n",
        "                       f\"{metric}_7d_avg\", f\"{metric}_30d_avg\", \"vs7_pct\", \"vs30_pct\"]\n",
        "        return pd.DataFrame(columns=cols)\n",
        "\n",
        "    c = current.groupby(keys, as_index=False)[metric].sum()\n",
        "    p = (prior.groupby(keys, as_index=False)[metric].sum()\n",
        "         if prior is not None and not prior.empty\n",
        "         else pd.DataFrame(columns=keys + [metric]))\n",
        "\n",
        "    m = c.merge(p, on=keys, how=\"left\", suffixes=(\"\", \"_yday\"))\n",
        "    ycol = f\"{metric}_yday\"\n",
        "    m[ycol] = m[ycol].fillna(0)\n",
        "    m[\"delta\"] = m[metric] - m[ycol]\n",
        "    m[\"pct_change\"] = np.where(m[ycol] != 0, (m[\"delta\"] / m[ycol]) * 100, np.nan)\n",
        "\n",
        "    # 7d / 30d baselines per key\n",
        "    h7 = hist[(hist[date_col] >= RUN_DATE - timedelta(days=7)) & (hist[date_col] < RUN_DATE)]\n",
        "    h30 = hist[(hist[date_col] >= RUN_DATE - timedelta(days=30)) & (hist[date_col] < RUN_DATE)]\n",
        "\n",
        "    h7g = (h7.groupby(keys + [date_col], as_index=False)[metric].sum()\n",
        "             .groupby(keys, as_index=False)[metric].mean()\n",
        "             .rename(columns={metric: f\"{metric}_7d_avg\"})) if not h7.empty else pd.DataFrame(columns=keys+[f\"{metric}_7d_avg\"])\n",
        "    h30g = (h30.groupby(keys + [date_col], as_index=False)[metric].sum()\n",
        "              .groupby(keys, as_index=False)[metric].mean()\n",
        "              .rename(columns={metric: f\"{metric}_30d_avg\"})) if not h30.empty else pd.DataFrame(columns=keys+[f\"{metric}_30d_avg\"])\n",
        "\n",
        "    m = m.merge(h7g, on=keys, how=\"left\").merge(h30g, on=keys, how=\"left\")\n",
        "\n",
        "    m[\"vs7_pct\"]  = np.where(m[f\"{metric}_7d_avg\"].fillna(0)  != 0, (m[metric] - m[f\"{metric}_7d_avg\"])  / m[f\"{metric}_7d_avg\"]  * 100, np.nan)\n",
        "    m[\"vs30_pct\"] = np.where(m[f\"{metric}_30d_avg\"].fillna(0) != 0, (m[metric] - m[f\"{metric}_30d_avg\"]) / m[f\"{metric}_30d_avg\"] * 100, np.nan)\n",
        "\n",
        "    m = m.sort_values(\"delta\", ascending=ascending)\n",
        "    return m.head(top_n) if top_n else m\n",
        "\n",
        "def _clean_for_join(s: pd.Series) -> pd.Series:\n",
        "    s = s.astype(str)\n",
        "    s = s.str.replace(r\"\\(.*?\\)\", \"\", regex=True)\n",
        "    s = s.str.replace(r\"grant[:\\- ]*\", \"\", regex=True)\n",
        "    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip().str.lower()\n",
        "    return s\n",
        "\n",
        "def compact_json(df: pd.DataFrame, cols: list[str], n=8):\n",
        "    if df is None or df.empty: return []\n",
        "    keep = [c for c in cols if c in df.columns]\n",
        "    return df.loc[:, keep].head(n).to_dict(orient=\"records\")\n"
      ],
      "metadata": {
        "id": "Gt_NJQOnuFnv"
      },
      "id": "Gt_NJQOnuFnv",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------- Parameterized queries (server-side date filter) ----------\n",
        "job_config = bigquery.QueryJobConfig(\n",
        "    query_parameters=[\n",
        "        bigquery.ScalarQueryParameter(\"start\", \"DATE\", LOOKBACK),\n",
        "        bigquery.ScalarQueryParameter(\"today\", \"DATE\", RUN_DATE),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# DAU\n",
        "df_dau = client.query(\"\"\"\n",
        "SELECT checkpoint AS Date, DAU, hours_played, avg_time_per_player\n",
        "FROM bigtimestudios.BT_ML.ai_bot_input_dau\n",
        "WHERE checkpoint BETWEEN @start AND @today\n",
        "\"\"\", job_config=job_config).to_dataframe()\n",
        "\n",
        "# Dungeons\n",
        "df_dungeon = client.query(\"\"\"\n",
        "SELECT Day, portalType, mission, Players, dungeonUptime, sandConsumed\n",
        "FROM bigtimestudios.BT_ML.ai_bot_input_dungeon\n",
        "WHERE Day BETWEEN @start AND @today\n",
        "\"\"\", job_config=job_config).to_dataframe()\n",
        "\n",
        "# Revenue\n",
        "df_revenue = client.query(\"\"\"\n",
        "SELECT Date, item_name, revenue, Buyers, transactions\n",
        "FROM bigtimestudios.BT_ML.ai_bot_input_revenue\n",
        "WHERE Date BETWEEN @start AND @today\n",
        "\"\"\", job_config=job_config).to_dataframe()\n",
        "\n",
        "# Tokens\n",
        "df_tokens = client.query(\"\"\"\n",
        "SELECT Day, action, sourceString, amount, users\n",
        "FROM bigtimestudios.BT_ML.ai_bot_input_token\n",
        "WHERE Day BETWEEN @start AND @today\n",
        "\"\"\", job_config=job_config).to_dataframe()\n",
        "\n",
        "# Crafts\n",
        "df_crafts = client.query(\"\"\"\n",
        "SELECT Day, name, crafts, users\n",
        "FROM bigtimestudios.BT_ML.ai_bot_input_crafts\n",
        "WHERE Day BETWEEN @start AND @today\n",
        "\"\"\", job_config=job_config).to_dataframe()\n"
      ],
      "metadata": {
        "id": "pVi_333nuIgh"
      },
      "id": "pVi_333nuIgh",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Normalize Dates ----------\n",
        "df_revenue[\"Date\"]  = ensure_date(df_revenue[\"Date\"])\n",
        "df_dau[\"Date\"]      = ensure_date(df_dau[\"Date\"])\n",
        "df_dungeon[\"Day\"]   = ensure_date(df_dungeon[\"Day\"])\n",
        "df_tokens[\"Day\"]    = ensure_date(df_tokens[\"Day\"])\n",
        "df_crafts[\"Day\"]    = ensure_date(df_crafts[\"Day\"])\n",
        "\n",
        "# ---------- Slices ----------\n",
        "rev_today = df_revenue.loc[df_revenue[\"Date\"] == RUN_DATE]\n",
        "rev_yday  = df_revenue.loc[df_revenue[\"Date\"] == YDAY]\n",
        "rev_7d    = df_revenue.loc[(df_revenue[\"Date\"] < RUN_DATE) & (df_revenue[\"Date\"] >= WEEK_AGO)]\n",
        "\n",
        "dau_today = df_dau.loc[df_dau[\"Date\"] == RUN_DATE]\n",
        "dau_yday  = df_dau.loc[df_dau[\"Date\"] == YDAY]\n",
        "dau_7d    = df_dau.loc[(df_dau[\"Date\"] < RUN_DATE) & (df_dau[\"Date\"] >= WEEK_AGO)]\n",
        "\n",
        "gp_today  = df_dungeon.loc[df_dungeon[\"Day\"] == RUN_DATE]\n",
        "gp_yday   = df_dungeon.loc[df_dungeon[\"Day\"] == YDAY]\n",
        "\n",
        "tok_today = df_tokens.loc[df_tokens[\"Day\"] == RUN_DATE]\n",
        "tok_yday  = df_tokens.loc[df_tokens[\"Day\"] == YDAY]\n",
        "\n",
        "cr_today  = df_crafts.loc[df_crafts[\"Day\"] == RUN_DATE]\n",
        "cr_yday   = df_crafts.loc[df_crafts[\"Day\"] == YDAY]"
      ],
      "metadata": {
        "id": "VbSjK7fAusuy"
      },
      "id": "VbSjK7fAusuy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- KPIs ----------\n",
        "total_rev_today = float(rev_today[\"revenue\"].sum()) if not rev_today.empty else 0.0\n",
        "buyers_today    = int(rev_today[\"Buyers\"].sum()) if not rev_today.empty else 0\n",
        "tx_today        = int(rev_today[\"transactions\"].sum()) if not rev_today.empty else 0\n",
        "total_rev_yday  = float(rev_yday[\"revenue\"].sum()) if not rev_yday.empty else 0.0\n",
        "\n",
        "rev_7d_avg  = window_avg(df_revenue, \"Date\", \"revenue\", RUN_DATE - timedelta(days=7),  RUN_DATE)\n",
        "rev_30d_avg = window_avg(df_revenue, \"Date\", \"revenue\", RUN_DATE - timedelta(days=30), RUN_DATE)\n",
        "\n",
        "rev_dod   = pct(total_rev_today, total_rev_yday)\n",
        "rev_vs7   = pct(total_rev_today, rev_7d_avg)\n",
        "rev_vs30  = pct(total_rev_today, rev_30d_avg)\n",
        "\n",
        "# DAU\n",
        "dau_today_v = int(dau_today[\"DAU\"].sum()) if not dau_today.empty else 0\n",
        "dau_yday_v  = int(dau_yday[\"DAU\"].sum()) if not dau_yday.empty else np.nan\n",
        "\n",
        "dau_7d_avg  = window_avg(df_dau, \"Date\", \"DAU\", RUN_DATE - timedelta(days=7),  RUN_DATE)\n",
        "dau_30d_avg = window_avg(df_dau, \"Date\", \"DAU\", RUN_DATE - timedelta(days=30), RUN_DATE)\n",
        "\n",
        "dau_dod  = pct(dau_today_v, dau_yday_v)\n",
        "dau_vs7  = pct(dau_today_v, dau_7d_avg)\n",
        "dau_vs30 = pct(dau_today_v, dau_30d_avg)\n",
        "\n",
        "summary_metrics = {\n",
        "    \"date\": RUN_DATE_STR,\n",
        "    \"revenue\": {\n",
        "        \"today\": round(total_rev_today, 2),\n",
        "        \"DoD_pct\": None if pd.isna(rev_dod) else round(100 * rev_dod, 2),\n",
        "        \"vs7d_avg_pct\": None if pd.isna(rev_vs7) else round(100 * rev_vs7, 2),\n",
        "        \"vs30d_avg_pct\": None if pd.isna(rev_vs30) else round(100 * rev_vs30, 2),\n",
        "        \"buyers\": buyers_today,\n",
        "        \"transactions\": tx_today,\n",
        "        \"avg_7d\": None if pd.isna(rev_7d_avg) else round(rev_7d_avg, 2),\n",
        "        \"avg_30d\": None if pd.isna(rev_30d_avg) else round(rev_30d_avg, 2),\n",
        "    },\n",
        "    \"dau\": {\n",
        "        \"today\": dau_today_v,\n",
        "        \"DoD_pct\": None if pd.isna(dau_dod) else round(100 * dau_dod, 2),\n",
        "        \"vs7d_avg_pct\": None if pd.isna(dau_vs7) else round(100 * dau_vs7, 2),\n",
        "        \"vs30d_avg_pct\": None if pd.isna(dau_vs30) else round(100 * dau_vs30, 2),\n",
        "        \"avg_7d\": None if pd.isna(dau_7d_avg) else round(dau_7d_avg, 2),\n",
        "        \"avg_30d\": None if pd.isna(dau_30d_avg) else round(dau_30d_avg, 2),\n",
        "        \"avg_time_today_min\": None if dau_today.empty else round(float(dau_today[\"avg_time_per_player\"].mean()), 2),\n",
        "    },\n",
        "}\n",
        "\n",
        "# Top seller (driver hint)\n",
        "if not rev_today.empty:\n",
        "    rev_by_item  = rev_today.groupby(\"item_name\", as_index=False)[\"revenue\"].sum()\n",
        "    top_item_row = rev_by_item.sort_values(\"revenue\", ascending=False).iloc[0]\n",
        "    top_item     = {\"item\": top_item_row[\"item_name\"], \"revenue\": float(top_item_row[\"revenue\"])}\n",
        "else:\n",
        "    top_item = None\n",
        "\n",
        "# ---------- Movers ----------\n",
        "gp_players_chg  = change_vs_prior(gp_today, gp_yday, df_dungeon, \"Day\",\n",
        "                                  [\"portalType\", \"mission\"], \"Players\", top_n=10)\n",
        "gp_sand_chg     = change_vs_prior(gp_today, gp_yday, df_dungeon, \"Day\",\n",
        "                                  [\"portalType\", \"mission\"], \"sandConsumed\", top_n=10)\n",
        "gp_players_drop = gp_players_chg.loc[gp_players_chg[\"delta\"] < 0].sort_values(\"delta\").head(5)\n",
        "\n",
        "tok_amt_chg   = change_vs_prior(tok_today, tok_yday, df_tokens, \"Day\",\n",
        "                                [\"action\", \"sourceString\"], \"amount\", top_n=15)\n",
        "tok_users_chg = change_vs_prior(tok_today, tok_yday, df_tokens, \"Day\",\n",
        "                                [\"action\", \"sourceString\"], \"users\", top_n=15)\n",
        "\n",
        "# ---------- Token sinks ----------\n",
        "sinks_today = (tok_today.loc[tok_today[\"action\"] == \"sink\"]\n",
        "               .groupby(\"sourceString\", as_index=False)[\"amount\"].sum()\n",
        "               .sort_values(\"amount\", ascending=False))\n",
        "tok_sinks_chg = change_vs_prior(\n",
        "    tok_today.loc[tok_today[\"action\"] == \"sink\"],\n",
        "    tok_yday.loc[tok_yday[\"action\"] == \"sink\"] if not tok_yday.empty else pd.DataFrame(columns=tok_today.columns),\n",
        "    df_tokens[df_tokens[\"action\"] == \"sink\"], \"Day\",\n",
        "    [\"sourceString\"], \"amount\", top_n=5\n",
        ")\n",
        "\n",
        "# Crafting\n",
        "cr_crafts_chg = change_vs_prior(cr_today, cr_yday, df_crafts, \"Day\",\n",
        "                                [\"name\"], \"crafts\", top_n=15)\n",
        "cr_users_chg  = change_vs_prior(cr_today, cr_yday, df_crafts, \"Day\",\n",
        "                                [\"name\"], \"users\",  top_n=15)\n",
        "# ---------- Mission-level grants (robust) ----------\n",
        "ctx_mission_insight = \"\"\n",
        "grants_today = tok_today.loc[tok_today[\"action\"] == \"grant\", [\"sourceString\", \"amount\"]].copy()\n",
        "gp_players_today = gp_today[[\"mission\", \"Players\"]].copy()\n",
        "\n",
        "if not grants_today.empty:\n",
        "    grants_today.rename(columns={\"sourceString\": \"mission_raw\"}, inplace=True)\n",
        "    grants_today[\"mission_norm\"] = _clean_for_join(grants_today[\"mission_raw\"])\n",
        "    token_grants_today = (grants_today.groupby(\"mission_norm\", as_index=False)[\"amount\"]\n",
        "                          .sum().rename(columns={\"amount\": \"tokens_granted\"}))\n",
        "else:\n",
        "    token_grants_today = pd.DataFrame(columns=[\"mission_norm\", \"tokens_granted\"])\n",
        "\n",
        "if not gp_players_today.empty:\n",
        "    gp_players_today[\"mission_norm\"] = _clean_for_join(gp_players_today[\"mission\"])\n",
        "    gp_players_today = gp_players_today.groupby(\"mission_norm\", as_index=False)[\"Players\"].sum()\n",
        "else:\n",
        "    gp_players_today = pd.DataFrame(columns=[\"mission_norm\", \"Players\"])\n",
        "\n",
        "mission_insights = token_grants_today.merge(gp_players_today, on=\"mission_norm\", how=\"inner\")\n",
        "\n",
        "if not mission_insights.empty:\n",
        "    top_m = mission_insights.sort_values(\"tokens_granted\", ascending=False).iloc[0]\n",
        "    if not gp_today.empty:\n",
        "        readable_map = (gp_today.assign(mission_norm=_clean_for_join(gp_today[\"mission\"]))\n",
        "                        .dropna(subset=[\"mission_norm\"])\n",
        "                        .drop_duplicates(subset=[\"mission_norm\"])\n",
        "                        .set_index(\"mission_norm\")[\"mission\"].to_dict())\n",
        "        readable_name = readable_map.get(top_m[\"mission_norm\"], str(top_m[\"mission_norm\"]))\n",
        "    else:\n",
        "        readable_name = str(top_m[\"mission_norm\"])\n",
        "    ctx_mission_insight = {\n",
        "        \"mission\": readable_name,\n",
        "        \"players\": int(top_m[\"Players\"]),\n",
        "        \"tokens_granted\": int(top_m[\"tokens_granted\"])\n",
        "    }\n",
        "elif not grants_today.empty:\n",
        "    top_grant = (grants_today.groupby(\"mission_raw\", as_index=False)[\"amount\"]\n",
        "                 .sum().sort_values(\"amount\", ascending=False).iloc[0])\n",
        "    ctx_mission_insight = {\n",
        "        \"mission\": str(top_grant[\"mission_raw\"]),\n",
        "        \"players\": None,\n",
        "        \"tokens_granted\": int(top_grant[\"amount\"]),\n",
        "        \"note\": \"mission link unavailable\"\n",
        "    }"
      ],
      "metadata": {
        "id": "m1jR9bIFu0F0"
      },
      "id": "m1jR9bIFu0F0",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Context (JSON-first, better for LLM reasoning) ----------\n",
        "ctx = {\n",
        "    \"date\": RUN_DATE_STR,\n",
        "    \"kpis\": summary_metrics,\n",
        "    \"drivers\": {\n",
        "        \"top_item\": top_item  # {\"item\": ..., \"revenue\": ...} or None\n",
        "    },\n",
        "    \"gameplay\": {\n",
        "        \"players_up\":   compact_json(gp_players_chg,  [\"portalType\",\"mission\",\"Players\",\"delta\",\"pct_change\",\"Players_7d_avg\",\"Players_30d_avg\",\"vs7_pct\",\"vs30_pct\"]),\n",
        "        \"players_down\": compact_json(gp_players_drop, [\"portalType\",\"mission\",\"Players\",\"delta\",\"pct_change\",\"Players_7d_avg\",\"Players_30d_avg\",\"vs7_pct\",\"vs30_pct\"]),\n",
        "        \"sand_movers\":  compact_json(gp_sand_chg,     [\"portalType\",\"mission\",\"sandConsumed\",\"delta\",\"pct_change\",\"sandConsumed_7d_avg\",\"sandConsumed_30d_avg\",\"vs7_pct\",\"vs30_pct\"]),\n",
        "        \"mission_insight\": ctx_mission_insight or {}\n",
        "    },\n",
        "    \"tokens\": {\n",
        "        \"amount_movers\": compact_json(tok_amt_chg, [\"action\",\"sourceString\",\"amount\",\"delta\",\"pct_change\",\"amount_7d_avg\",\"amount_30d_avg\",\"vs7_pct\",\"vs30_pct\"]),\n",
        "        \"user_movers\":   compact_json(tok_users_chg, [\"action\",\"sourceString\",\"users\",\"delta\",\"pct_change\",\"users_7d_avg\",\"users_30d_avg\",\"vs7_pct\",\"vs30_pct\"]),\n",
        "        \"sinks_today\":   compact_json(sinks_today, [\"sourceString\",\"amount\"]),\n",
        "        \"sinks_change\":  compact_json(tok_sinks_chg, [\"sourceString\",\"amount\",\"delta\",\"pct_change\",\"amount_7d_avg\",\"amount_30d_avg\",\"vs7_pct\",\"vs30_pct\"])\n",
        "    },\n",
        "    \"crafting\": {\n",
        "        \"craft_movers\": compact_json(cr_crafts_chg, [\"name\",\"crafts\",\"delta\",\"pct_change\",\"crafts_7d_avg\",\"crafts_30d_avg\",\"vs7_pct\",\"vs30_pct\"]),\n",
        "        \"user_movers\":  compact_json(cr_users_chg,  [\"name\",\"users\",\"delta\",\"pct_change\",\"users_7d_avg\",\"users_30d_avg\",\"vs7_pct\",\"vs30_pct\"])\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "ooCKZE2ju3lf"
      },
      "id": "ooCKZE2ju3lf",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Prompt ----------\n",
        "def format_prompt(ctx):\n",
        "    return (\n",
        "f\"\"\"You are Head of Analytics for Role Playing Game 'Big Time'.\n",
        "\n",
        "Game context (short): Players buy Time Crystals (TC), use TC to add Time Sand to Hourglasses, then run Prestige Portals to farm $BIGTIME (e.g., 'racial' drops). Users also use TC to open Epoch Chests, which have been the main way we issue $BIGTIME to players over the last year. Entry to prestige uses $BIGTIME + TC. 'grant' in tokens often maps to a dungeon mission.\n",
        "\n",
        "Using the JSON below, write a concise daily update for {ctx['date']}.\n",
        "Rules:\n",
        "- Bullets only, no preamble.\n",
        "- For each section, state the change and one likely driver.\n",
        "- Use both baselines: vs 7d and vs 30d.\n",
        "- Call out anomalies (large % moves, issuance>rev, farming concentration).\n",
        "\n",
        "JSON:\n",
        "{json.dumps(ctx, separators=(',',':'))}\n",
        "\"\"\").strip()"
      ],
      "metadata": {
        "id": "aO9JqFWGu8JZ"
      },
      "id": "aO9JqFWGu8JZ",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize\n",
        "def summarize_with_vertex(prompt_text: str) -> str:\n",
        "    return model.generate_content(prompt_text).text\n",
        "\n",
        "prompt_text = format_prompt(ctx)\n",
        "ai_summary = summarize_with_vertex(prompt_text)\n",
        "\n",
        "# ---------- Fallback (simple bullets) ----------\n",
        "if not ai_summary:\n",
        "    lines = []\n",
        "    lines.append(f\"**Daily Update — {RUN_DATE_STR}**\")\n",
        "    lines.append(f\"- Revenue: ${summary_metrics['revenue']['today']:,.0f} \"\n",
        "                 f\"({summary_metrics['revenue']['DoD_pct']}% DoD, {summary_metrics['revenue']['vs7d_avg_pct']}% vs 7d); \"\n",
        "                 f\"buyers={summary_metrics['revenue']['buyers']:,}, tx={summary_metrics['revenue']['transactions']:,}\")\n",
        "    lines.append(f\"- DAU: {summary_metrics['dau']['today']:,} \"\n",
        "                 f\"({summary_metrics['dau']['DoD_pct']}% DoD, {summary_metrics['dau']['vs7d_avg_pct']}% vs 7d); \"\n",
        "                 f\"avg time={summary_metrics['dau']['avg_time_today_min']} min\")\n",
        "    ai_summary = \"\\n\".join(lines)\n",
        "\n",
        "print(\"\\n===== AI SUMMARY =====\\n\")\n",
        "print(ai_summary[:4000])\n"
      ],
      "metadata": {
        "id": "pRM9-4SBr94m"
      },
      "id": "pRM9-4SBr94m",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "matthew.thwaites (Nov 7, 2025, 12:58:53 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}